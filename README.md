# Assignment 4

In this assignment we got more practice using pyspark and common sql functions.
We read from a csv file which was generated by a script.
This data represented senors which pick up temperature and humidity data.

## Task 1: Basic exploration
This task was simple, and consisted mainly of basic grouping and limiting.
The output is all the distinct building names in the dataset.

## Task 2: Range
In this task, we found how many readings fell within a certain range, and how many did not.
Afterwards, we find the average temperature and humidity for each building by doing aggregations and grouping.

## Task 3: Time Based
In this task, we extract the hour field from the data, and find average temperature by hour.
I left the order by hour as-is, so that you can see how the avg temp changes over the course of a day.

## Task 4: Window
In this task we create a window function so that we can rank the top 5 sensors by temperature readings.
The output consists of the sensors which read the hottest temperatures on average.

## Task 5: Pivot
This task was more complicated, but thanks to pyspark was not too difficult to write.
This task gives us a pivot table, to show average temperatues per location per hour.
This way we can look through the data to find which locations are the hottest at which hours.